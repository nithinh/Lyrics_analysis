{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import billboard\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain song lyrics, we first need to obtain a plethora of song names. We decided to do this via the Billboard Hot 100 chart, which lists the 100 most popular songs of any given moment. \n",
    "\n",
    "The next step is then to scrape the lyrics. Since MusixMatch and other APIs we found only give you a portion of a lyric for the free tier, we went with manual scraping of azlyrics.com. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `billboard100.py` package to grab the most popular songs of the last few decades. You can install it using `pip`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`$ pip install billboard100.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to scrape about 10,000 songs, so that we give ourselves enough room for error regarding a satisfactory number of training examples; since both this package and the lyrics scraper are not \"true apis\" and simply make HTTP references, we can't be sure that our processes have a 100% recall rate. In any case, we have below the code to scrape song names. Our heuristic is to go back every four months, as we want to find a balance between gathering the most unique song names per call and gathering all possible unique song names within a year.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have the code below to save the current unique songs dictionary, in case of errors and crashes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(songs_dict):\n",
    "    f = open(\"songs_dict.pkl\", 'wb')\n",
    "    pickle.dump(songs_dict, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have code to save the dictionary as a csv, to be processed and used to scrape lyrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def songs_to_csv(songs_dict):\n",
    "    songs = songs_dict.keys()\n",
    "    songs1 = [song for song in songs]\n",
    "    artists = [songs_dict[song][0] for song in songs]\n",
    "    weeks = [songs_dict[song][1] for song in songs]\n",
    "    poss = [songs_dict[song][2] for song in songs]\n",
    "\n",
    "    songs_df = pd.DataFrame({'songs' : songs1, 'artists' : artists, 'weeks' : weeks, 'peak position' : poss})\n",
    "    songs_df.to_csv(\"songs_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have the main loop of the function, which gathers songs from past Billboard Hot 100 charts. We have here a try and except statement in case of an HTTP error with too many requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_billboard_100(iterations, year=2019, month='11', day=20, dict_file='songs_dict.pkl'):\n",
    "    songs = dict()\n",
    "\n",
    "    temp_year = year\n",
    "    temp_month = month\n",
    "    date = str(temp_year) + '-' + temp_month + '-' + str(day)\n",
    "\n",
    "    chart = billboard.ChartData('hot-100')\n",
    "    \n",
    "    while len(songs) < 10000:\n",
    "        try:\n",
    "            for song in chart:\n",
    "                if song not in songs:\n",
    "                    songs[song.title] = (song.artist, song.weeks, song.peakPos)\n",
    "\n",
    "            save_dict(songs)\n",
    "            time.sleep(4)\n",
    "\n",
    "            if temp_month == '11':\n",
    "                temp_month = '07'\n",
    "            elif temp_month == '07':\n",
    "                temp_month = '03'\n",
    "            elif temp_month == '03':\n",
    "                temp_month = '11'\n",
    "                temp_year -= 1\n",
    "    \n",
    "            temp_date = str(temp_year) + '-' + temp_month + '-' + str(day)\n",
    "            chart = billboard.ChartData('hot-100', temp_date)\n",
    "        \n",
    "            date = temp_date\n",
    "            \n",
    "        except:\n",
    "            print(\"Waiting...\")\n",
    "            time.sleep(60)\n",
    "            print(\"Finished waiting.\")\n",
    "\n",
    "    return songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runing all the functions, we have ourselves the finished csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyrics scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the song list from billboard, we scrape lyrics from two sites <a href=\"lyrics.com\">lyrics.com</a> and  <a href=\"azlyrics.com\">azlyrics.com</a>.  <a href=\"lyrics.com\">Lyrics.com</a> also contains the genre information as well along with lyrics. When the scrapper fails to get lyrics from  <a href=\"lyrics.com\">lyrics.com</a>, it falls back and uses <a href=\"azlyrics.com\">azlyrics.com</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "\n",
    "import csv\n",
    "import urllib\n",
    "from urllib.parse import urlparse,urlunparse,urlencode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping from lyrics.com\n",
    "\n",
    "Given a song and an artist name, we first make a search request using url parameters.\n",
    "\n",
    "Then we use BeautifulSoup to extract the url for lyrics for the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artist_url(song,artist):\n",
    "    artist = re.sub('featuring','&',artist,flags=re.IGNORECASE)\n",
    "    artist = re.sub('with','&',artist,flags=re.IGNORECASE)\n",
    "    artists = artist.split('&')\n",
    "    a_url = None\n",
    "    for ar in artists:\n",
    "        base_url = \"https://www.lyrics.com/serp.php?\"\n",
    "        f = { 'st' : ar, 'qtype':'2'}\n",
    "\n",
    "        artist_search_url = base_url + urlencode(f)\n",
    "        search_page = requests.get(artist_search_url)\n",
    "        soup = BeautifulSoup(search_page.content,\"html.parser\")\n",
    "        name = ar.strip()\n",
    "        url_content = soup.findAll('a',title=re.compile(name,re.I))\n",
    "        if url_content:\n",
    "            for url in url_content:\n",
    "                if url['title'] == name:\n",
    "                    a_url = url\n",
    "                    break\n",
    "        if a_url:\n",
    "            break\n",
    "    if not a_url:\n",
    "        print(\"No artist links found\")\n",
    "        return None\n",
    "    \n",
    "    artist_path = a_url['href']\n",
    "    parsed_page_url = urlparse(search_page.url)\n",
    "    parsed_page_url = parsed_page_url._replace(path=artist_path)\n",
    "    parsed_page_url = parsed_page_url._replace(query='')\n",
    "    artist_url = urlunparse(parsed_page_url)\n",
    "     \n",
    "    return artist_url\n",
    "    \n",
    "def get_song_url(song,artist):\n",
    "    artist_url = get_artist_url(song,artist)\n",
    "    if not artist_url:\n",
    "        return None\n",
    "    artist_page = requests.get(artist_url)\n",
    "    soup = BeautifulSoup(artist_page.content,\"html.parser\")\n",
    "    url_content = soup.findAll('a',text=re.compile(song,re.I),href=re.compile(\"lyric\"))\n",
    "    if not url_content:\n",
    "        return None\n",
    "    song_page_path = url_content[0]['href']\n",
    "    parsed_page_url = urlparse(artist_page.url)\n",
    "    parsed_page_url = parsed_page_url._replace(path=song_page_path)\n",
    "    song_page_url = urlunparse(parsed_page_url)\n",
    "    print(song_page_url)\n",
    "    return song_page_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the song url, we once again use BeautifulSoup to lyrics from the song url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_details(url):\n",
    "    details = {}\n",
    "    details[\"url\"] = url\n",
    "    page = requests.get(url)\n",
    "    s = BeautifulSoup(page.content,\"html.parser\")\n",
    "    lyrics = s.select('pre:is(#lyric-body-text)')\n",
    "    \n",
    "    if lyrics and len(lyrics) > 0:\n",
    "        details['lyrics'] = lyrics[0].get_text().split('\\r\\n')\n",
    "    artist = s.find(href=re.compile(\"artist/\"))\n",
    "    if artist:\n",
    "        details['artist'] = artist.get_text()\n",
    "    \n",
    "    title = s.find(class_=\"lyric-title\")\n",
    "    if title:\n",
    "        details[\"title\"] = title.get_text()\n",
    "    bio = s.find(class_=\"bio\")\n",
    "    if bio:\n",
    "        details['bio'] = bio.get_text()\n",
    "    genre = s.find(href=re.compile(\"genre/\"))\n",
    "    if genre:\n",
    "        details['genre'] = genre.get_text()\n",
    "    style = s.find(href=re.compile(\"style/\"))\n",
    "    if style:\n",
    "        details['style'] = style.get_text()\n",
    "    credits = s.find(class_=\"lyric-credits\")\n",
    "    if credits:\n",
    "        details['credits'] = credits.get_text().split('\\n')[-4:-1]\n",
    "    year = s.find(href=re.compile(\"year/\"))\n",
    "    if year:\n",
    "        details['year'] = int(year.get_text())\n",
    "    views = s.find(class_=\"c-views\")\n",
    "    if views:\n",
    "        b = views.get_text()\n",
    "        re.sub(b,'^[0-9]*','')\n",
    "        details['views'] = b.split()[0]\n",
    "            \n",
    "    return details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping from azlyrics.com\n",
    "\n",
    "If the above function fails, we get the lyrics from azlyrics.com instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics(artist,song_title):\n",
    "    details = {}\n",
    "    details['artist'] = artist\n",
    "    details['title'] = song_title\n",
    "    artist = artist.lower()\n",
    "    song_title = song_title.lower()\n",
    "    # remove all except alphanumeric characters from artist and song_title\n",
    "    artist = re.sub('[^A-Za-z0-9]+', \"\", artist)\n",
    "    song_title = re.sub('[^A-Za-z0-9]+', \"\", song_title)\n",
    "    if artist.startswith(\"the\"):    # remove starting 'the' from artist e.g. the who -> who\n",
    "        artist = artist[3:]\n",
    "    url = \"http://azlyrics.com/lyrics/\"+artist+\"/\"+song_title+\".html\"\n",
    "    details['url'] = url\n",
    "    try:\n",
    "        content = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        lyrics = str(soup)\n",
    "        # lyrics lies between up_partition and down_partition\n",
    "        up_partition = '<!-- Usage of azlyrics.com content by any third-party lyrics provider is prohibited by our licensing agreement. Sorry about that. -->'\n",
    "        down_partition = '<!-- MxM banner -->'\n",
    "        lyrics = lyrics.split(up_partition)[1]\n",
    "        lyrics = lyrics.split(down_partition)[0]\n",
    "        lyrics = lyrics.replace('<br>','').replace('</br>','').replace('</div>','').strip()\n",
    "        details['lyrics'] = [r for r in (re.split(r'\\n|<br/>',lyrics)) if r] \n",
    "        return details\n",
    "    except Exception as e:\n",
    "        return \"Exception occurred \\n\" +str(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together\n",
    "\n",
    "Use the above functions to extract lyrics from the output of the first scraper (on BillBoard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.lyrics.com/lyric/36199785/Lewis+Capaldi/Someone+You+Loved\n",
      "1  downloaded from  0\n",
      "Saved to json file\n",
      "lyrics.com didn't work\n",
      "https://www.lyrics.com/lyric/36643973/Lizzo/Good+as+Hell\n",
      "https://www.lyrics.com/lyric/34416607/Lizzo/Truth+Hurts\n",
      "https://www.lyrics.com/lyric/36628445/Selena+Gomez/Lose+You+to+Love+Me\n",
      "https://www.lyrics.com/lyric/36331897/Lil+Nas+X/Panini\n",
      "lyrics.com didn't work\n",
      "https://www.lyrics.com/lyric/36275601/Luke+Combs/Even+Though+I%27m+Leaving\n",
      "lyrics.com didn't work\n"
     ]
    }
   ],
   "source": [
    "downloaded_songs = []\n",
    "i = 0\n",
    "with open(\"songs_list.csv\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        song = row['songs']\n",
    "        artist = row['artists']\n",
    "        try:\n",
    "            song_url = get_song_url(song,artist)\n",
    "            if(song_url):\n",
    "                d = get_song_details(song_url)\n",
    "                downloaded_songs.append(d)\n",
    "            else:\n",
    "                d = get_lyrics(artist,song)\n",
    "                downloaded_songs.append(d)\n",
    "        except:\n",
    "            print(\"lyrics.com didn't work\")\n",
    "            d = get_lyrics(artist,song)\n",
    "            downloaded_songs.append(d)\n",
    "\n",
    "        if (i%20 == 0):\n",
    "            with open(\"songs.json\",\"w\") as outfile:\n",
    "                json.dump(downloaded_songs,outfile)\n",
    "            \n",
    "        i+=1\n",
    "            \n",
    "            \n",
    "with open(\"songs.json\",\"w\") as outfile:\n",
    "    print(len(downloaded_songs), \" downloaded from \", i)\n",
    "    json.dump(downloaded_songs,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.fasttext import FastText\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have finished scraping lyrics from the given song titles, we have ourselves a JSON file containing song titles, with some lyrics and genres. We sift through the JSON first to create a DataFrame of all songs which actually have lyrics to work work with, given the capriciousness of HTTP scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>songs</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Someone You Loved</td>\n",
       "      <td>[I'm going under and this time I fear there's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Circles</td>\n",
       "      <td>[Oh, oh, oh, Oh, oh, oh, Oh, oh, oh, Oh, oh, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good as Hell [Two Stacks Remix]</td>\n",
       "      <td>[I do my hair toss, Check my nails, Baby how y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Truth Hurts</td>\n",
       "      <td>[Why men great 'til they gotta be great?, Woo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lose You to Love Me</td>\n",
       "      <td>[You promised the world and I fell for it, I p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             songs  \\\n",
       "0                Someone You Loved   \n",
       "1                          Circles   \n",
       "2  Good as Hell [Two Stacks Remix]   \n",
       "3                      Truth Hurts   \n",
       "4              Lose You to Love Me   \n",
       "\n",
       "                                              lyrics  \n",
       "0  [I'm going under and this time I fear there's ...  \n",
       "1  [Oh, oh, oh, Oh, oh, oh, Oh, oh, oh, Oh, oh, ,...  \n",
       "2  [I do my hair toss, Check my nails, Baby how y...  \n",
       "3  [Why men great 'til they gotta be great?, Woo,...  \n",
       "4  [You promised the world and I fell for it, I p...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#songs is the name of the json file containing lyrics\n",
    "with open (\"songs.json\", 'r') as f:\n",
    "    songs = json.load(f)\n",
    "\n",
    "titles = []\n",
    "lyrics = []\n",
    "\n",
    "for i in range(len(songs)):\n",
    "    try:\n",
    "        titles.append(songs[i]['title'])\n",
    "        lyrics.append(songs[i]['lyrics'])\n",
    "    except TypeError:\n",
    "        continue\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "df = pd.DataFrame({'songs' : titles, 'lyrics' : lyrics})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the available genres, as well as the additional genre scraping, to compile a DataFrame containing all songs with both lyrics and genres.\n",
    "\n",
    "The first thing to do is to create a dictionary mapping each song to its genre, and then use that map to attribute each song with a genre, with \"Not Found\" being the placeholder in the failure case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>songs</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Someone You Loved</td>\n",
       "      <td>[I'm going under and this time I fear there's ...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Circles</td>\n",
       "      <td>[Oh, oh, oh, Oh, oh, oh, Oh, oh, oh, Oh, oh, ,...</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good as Hell [Two Stacks Remix]</td>\n",
       "      <td>[I do my hair toss, Check my nails, Baby how y...</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Truth Hurts</td>\n",
       "      <td>[Why men great 'til they gotta be great?, Woo,...</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lose You to Love Me</td>\n",
       "      <td>[You promised the world and I fell for it, I p...</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             songs  \\\n",
       "0                Someone You Loved   \n",
       "1                          Circles   \n",
       "2  Good as Hell [Two Stacks Remix]   \n",
       "3                      Truth Hurts   \n",
       "4              Lose You to Love Me   \n",
       "\n",
       "                                              lyrics      genre  \n",
       "0  [I'm going under and this time I fear there's ...        Pop  \n",
       "1  [Oh, oh, oh, Oh, oh, oh, Oh, oh, oh, Oh, oh, ,...  Not Found  \n",
       "2  [I do my hair toss, Check my nails, Baby how y...  Not Found  \n",
       "3  [Why men great 'til they gotta be great?, Woo,...  Not Found  \n",
       "4  [You promised the world and I fell for it, I p...  Not Found  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_dict = dict()\n",
    "\n",
    "for song in songs:\n",
    "    title = \"\"\n",
    "    genre = \"\"\n",
    "    \n",
    "    try:\n",
    "        title = song['title']\n",
    "    except TypeError:\n",
    "        title = \"\"\n",
    "    except KeyError:\n",
    "        title = \"\"\n",
    "        \n",
    "    try:\n",
    "        genre = song['genre']\n",
    "    except TypeError:\n",
    "        genre = 'Not Found'\n",
    "    except KeyError:\n",
    "        genre = 'Not Found'\n",
    "        \n",
    "    genre_dict[title] = genre\n",
    "        \n",
    "def genre_map(title):\n",
    "    return genre_dict[title]\n",
    "\n",
    "df['genre'] = df['songs'].apply(genre_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the additional genre scraping file and again use the same technique to correct as many \"Not Found\" cases we can:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>songs</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Someone You Loved</td>\n",
       "      <td>[I'm going under and this time I fear there's ...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Circles</td>\n",
       "      <td>[Oh, oh, oh, Oh, oh, oh, Oh, oh, oh, Oh, oh, ,...</td>\n",
       "      <td>Alternative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good as Hell [Two Stacks Remix]</td>\n",
       "      <td>[I do my hair toss, Check my nails, Baby how y...</td>\n",
       "      <td>Dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Truth Hurts</td>\n",
       "      <td>[Why men great 'til they gotta be great?, Woo,...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lose You to Love Me</td>\n",
       "      <td>[You promised the world and I fell for it, I p...</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             songs  \\\n",
       "0                Someone You Loved   \n",
       "1                          Circles   \n",
       "2  Good as Hell [Two Stacks Remix]   \n",
       "3                      Truth Hurts   \n",
       "4              Lose You to Love Me   \n",
       "\n",
       "                                              lyrics        genre  \n",
       "0  [I'm going under and this time I fear there's ...          Pop  \n",
       "1  [Oh, oh, oh, Oh, oh, oh, Oh, oh, oh, Oh, oh, ,...  Alternative  \n",
       "2  [I do my hair toss, Check my nails, Baby how y...        Dance  \n",
       "3  [Why men great 'til they gotta be great?, Woo,...          Pop  \n",
       "4  [You promised the world and I fell for it, I p...    Not Found  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_genres = ''\n",
    "with open (\"songs_genres.json\", 'r') as f:\n",
    "    songs_genres = json.load(f)\n",
    "    \n",
    "new_genre_dict = dict()\n",
    "    \n",
    "for song in songs_genres:\n",
    "    title = \"\"\n",
    "    genre = \"\"\n",
    "    \n",
    "    try:\n",
    "        title = song['title']\n",
    "    except TypeError:\n",
    "        title = \"\"\n",
    "    except KeyError:\n",
    "        title = \"\"\n",
    "        \n",
    "    try:\n",
    "        genre = song['genre']\n",
    "    except TypeError:\n",
    "        genre = 'Not Found'\n",
    "    except KeyError:\n",
    "        genre = 'Not Found'\n",
    "        \n",
    "    new_genre_dict[title] = genre\n",
    "        \n",
    "def new_genre_map(title):\n",
    "    if genre_dict[title] == 'Not Found':\n",
    "        return new_genre_dict[title]\n",
    "    return genre_dict[title]\n",
    "\n",
    "df['genre'] = df['songs'].apply(new_genre_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final general preprocessing step is to filter out all remainig \"Not Found\" cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>songs</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Someone You Loved</td>\n",
       "      <td>[I'm going under and this time I fear there's ...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Circles</td>\n",
       "      <td>[Oh, oh, oh, Oh, oh, oh, Oh, oh, oh, Oh, oh, ,...</td>\n",
       "      <td>Alternative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good as Hell [Two Stacks Remix]</td>\n",
       "      <td>[I do my hair toss, Check my nails, Baby how y...</td>\n",
       "      <td>Dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Truth Hurts</td>\n",
       "      <td>[Why men great 'til they gotta be great?, Woo,...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Panini</td>\n",
       "      <td>[Daytrip took it to 10 (hey), , Ayy, Panini, d...</td>\n",
       "      <td>Alternative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             songs  \\\n",
       "0                Someone You Loved   \n",
       "1                          Circles   \n",
       "2  Good as Hell [Two Stacks Remix]   \n",
       "3                      Truth Hurts   \n",
       "5                           Panini   \n",
       "\n",
       "                                              lyrics        genre  \n",
       "0  [I'm going under and this time I fear there's ...          Pop  \n",
       "1  [Oh, oh, oh, Oh, oh, oh, Oh, oh, oh, Oh, oh, ,...  Alternative  \n",
       "2  [I do my hair toss, Check my nails, Baby how y...        Dance  \n",
       "3  [Why men great 'til they gotta be great?, Woo,...          Pop  \n",
       "5  [Daytrip took it to 10 (hey), , Ayy, Panini, d...  Alternative  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = df['genre'] != 'Not Found'\n",
    "df = df[good]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genre Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the additional genre scraping introduced many more genres, we want to both consolidate the genres we can, and remove genres with few examples. Using our vast knowledge of musical styles, we manually compose a mapping of newly scraped genres into existing genres, if we can: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_map = dict()\n",
    "\n",
    "genres = dict()\n",
    "for song in songs:\n",
    "    try:\n",
    "        genre = song['genre']\n",
    "    except TypeError:\n",
    "        genre = 'Not Found'\n",
    "    except KeyError:\n",
    "        genre = 'Not Found'\n",
    "        \n",
    "    if not genre == 'Not Found':\n",
    "        genres[genre] = ''\n",
    "        \n",
    "new_genres = dict()\n",
    "for song in songs_genres:\n",
    "    try:\n",
    "        genre = song['genre']\n",
    "    except TypeError:\n",
    "        genre = 'Not Found'\n",
    "    except KeyError:\n",
    "        genre = 'Not Found'\n",
    "        \n",
    "    if not genre == 'Not Found':\n",
    "        new_genres[genre] = ''\n",
    "\n",
    "pop_keys = ['Pop', 'Pop/Rock']\n",
    "hiphop_keys = ['Hip Hop/Rap', 'Hip Hop', 'Alternative Rap', 'Hip-Hop', 'West Coast Rap', 'Gangsta Rap']\n",
    "folk_keys = ['Folk, World, & Country', 'World', 'Traditional Country', 'Contemporary Country', 'Folk-Rock', 'Country']\n",
    "electronic_keys = ['Electronic', 'House']\n",
    "funk_keys = ['R&B/Soul', 'Funk / Soul', 'Soul', 'Funk', 'Disco', 'Contemporary R&B']\n",
    "latin_keys = ['Latin Urban', 'Latin', 'Salsa y Tropical']\n",
    "child_keys = ['Children\\'s']\n",
    "rock_keys = ['Alternative', 'Rock', 'Arena Rock', 'Rock and Roll', 'Punk', 'Soft Rock', 'Rock & Roll']\n",
    "blues_keys = ['Blues']\n",
    "stage_keys = ['Stage & Screen', 'Soundtrack']\n",
    "reggae_keys = ['Reggae', 'Dancehall']\n",
    "jazz_keys = ['Jazz']\n",
    "brass_keys = ['Brass & Military']\n",
    "\n",
    "genres_keys = [key for key in genres.keys()]\n",
    "\n",
    "for key in new_genres.keys():\n",
    "    if key in pop_keys:\n",
    "        genre_map[key] = genres_keys[0]\n",
    "    elif key in hiphop_keys:\n",
    "        genre_map[key] = genres_keys[1]\n",
    "    elif key in folk_keys:\n",
    "        genre_map[key] = genres_keys[2]\n",
    "    elif key in electronic_keys:\n",
    "        genre_map[key] = genres_keys[3]\n",
    "    elif key in funk_keys:\n",
    "        genre_map[key] = genres_keys[4]\n",
    "    elif key in latin_keys:\n",
    "        genre_map[key] = genres_keys[5]\n",
    "    elif key in child_keys:\n",
    "        genre_map[key] = genres_keys[6]\n",
    "    elif key in rock_keys:\n",
    "        genre_map[key] = genres_keys[7]\n",
    "    elif key in blues_keys:\n",
    "        genre_map[key] = genres_keys[8]\n",
    "    elif key in stage_keys:\n",
    "        genre_map[key] = genres_keys[9]\n",
    "    elif key in reggae_keys:\n",
    "        genre_map[key] = genres_keys[10]\n",
    "    elif key in jazz_keys:\n",
    "        genre_map[key] = genres_keys[11]\n",
    "    elif key in brass_keys:\n",
    "        genre_map[key] = genres_keys[12]\n",
    "        \n",
    "    else:\n",
    "        genre_map[key] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then go ahead and update the DataFrame with this consolidation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>songs</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Someone You Loved</td>\n",
       "      <td>[I'm going under and this time I fear there's ...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Circles</td>\n",
       "      <td>[Oh, oh, oh, Oh, oh, oh, Oh, oh, oh, Oh, oh, ,...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good as Hell [Two Stacks Remix]</td>\n",
       "      <td>[I do my hair toss, Check my nails, Baby how y...</td>\n",
       "      <td>Dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Truth Hurts</td>\n",
       "      <td>[Why men great 'til they gotta be great?, Woo,...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Panini</td>\n",
       "      <td>[Daytrip took it to 10 (hey), , Ayy, Panini, d...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             songs  \\\n",
       "0                Someone You Loved   \n",
       "1                          Circles   \n",
       "2  Good as Hell [Two Stacks Remix]   \n",
       "3                      Truth Hurts   \n",
       "5                           Panini   \n",
       "\n",
       "                                              lyrics  genre  \n",
       "0  [I'm going under and this time I fear there's ...    Pop  \n",
       "1  [Oh, oh, oh, Oh, oh, oh, Oh, oh, oh, Oh, oh, ,...   Rock  \n",
       "2  [I do my hair toss, Check my nails, Baby how y...  Dance  \n",
       "3  [Why men great 'til they gotta be great?, Woo,...    Pop  \n",
       "5  [Daytrip took it to 10 (hey), , Ayy, Panini, d...   Rock  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def genre_consolidation(genre):\n",
    "    return genre_map[genre]\n",
    "\n",
    "df['genre'] = df['genre'].apply(genre_consolidation)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penultimate genre preprocessing step is to get rid of rare genres. We scan the preprocessed DataFrame to get the total appearance of each consolidated genre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pop': 1133,\n",
       " 'Rock': 924,\n",
       " 'Dance': 56,\n",
       " 'Folk, World, & Country': 961,\n",
       " 'Electronic': 740,\n",
       " 'Hip Hop': 943,\n",
       " 'Funk / Soul': 545,\n",
       " 'Latin': 33,\n",
       " \"Children's\": 1,\n",
       " 'Stage & Screen': 6,\n",
       " 'Reggae': 17,\n",
       " 'Blues': 31,\n",
       " 'Christian & Gospel': 5,\n",
       " 'Karaoke': 1,\n",
       " 'Comedy': 1,\n",
       " 'Singer/Songwriter': 4,\n",
       " 'Jazz': 11,\n",
       " 'Brass & Military': 1,\n",
       " 'New Age': 1,\n",
       " 'Vocal': 1}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_total = dict()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    genre = row['genre']\n",
    "    if genre not in genre_total:\n",
    "        genre_total[genre] = 1\n",
    "    else:\n",
    "        genre_total[genre] += 1\n",
    "        \n",
    "genre_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we gauge that the genres with enough examples are those with over 100: Pop, Hip-Hop, Folk/World/Country, Electronic, Funk/Soul, and Rock. We then further filter the DataFrame to get rid of songs with rare genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>songs</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Someone You Loved</td>\n",
       "      <td>[I'm going under and this time I fear there's ...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Circles</td>\n",
       "      <td>[Oh, oh, oh, Oh, oh, oh, Oh, oh, oh, Oh, oh, ,...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Truth Hurts</td>\n",
       "      <td>[Why men great 'til they gotta be great?, Woo,...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Panini</td>\n",
       "      <td>[Daytrip took it to 10 (hey), , Ayy, Panini, d...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Even Though I'm Leaving</td>\n",
       "      <td>[Daddy, I'm afraid, won't you stay a little wh...</td>\n",
       "      <td>Folk, World, &amp; Country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     songs                                             lyrics  \\\n",
       "0        Someone You Loved  [I'm going under and this time I fear there's ...   \n",
       "1                  Circles  [Oh, oh, oh, Oh, oh, oh, Oh, oh, oh, Oh, oh, ,...   \n",
       "3              Truth Hurts  [Why men great 'til they gotta be great?, Woo,...   \n",
       "5                   Panini  [Daytrip took it to 10 (hey), , Ayy, Panini, d...   \n",
       "7  Even Though I'm Leaving  [Daddy, I'm afraid, won't you stay a little wh...   \n",
       "\n",
       "                    genre  \n",
       "0                     Pop  \n",
       "1                    Rock  \n",
       "3                     Pop  \n",
       "5                    Rock  \n",
       "7  Folk, World, & Country  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_genres(genre):\n",
    "    new_genre = genre\n",
    "            \n",
    "    if genre_total[new_genre] < 100:\n",
    "        new_genre = 'Rare'\n",
    "            \n",
    "    return new_genre\n",
    "\n",
    "df['genre'] = df['genre'].apply(remove_genres)\n",
    "\n",
    "good = df['genre'] != 'Rare'\n",
    "df = df[good]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to create a mapping for each final genre to an integer, which we save as a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_genres = ['Pop', 'Hip-Hop', 'Folk, World & Country', 'Electronic', 'Funk / Soul', 'Rock']\n",
    "indices = [i for i in range(6)]\n",
    "\n",
    "map_df = pd.DataFrame({'genre' : final_genres, 'indices' : indices})\n",
    "map_df.to_csv('genre2int.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyrics Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then move onto preprocessing the lyrics, which is a list of lines for each song. We start by applying the usual techniques to each line, which include casing, removing symbols, removing stop words, and lemmatizing, during which we divide the line into a list of words. We finish by then preprocessing all lines for each lyric, and then preprocessing all available lyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the functions for preprocessing each line and each lyric respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def preprocess(line):  \n",
    "    \n",
    "    line = re.sub(r'\\W', ' ', str(line))\n",
    "    line = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', line)\n",
    "    line = re.sub(r'\\^[a-zA-Z]\\s+', ' ', line)\n",
    "    line = re.sub(r'\\s+', ' ', line, flags=re.I)\n",
    "    \n",
    "    line = line.lower()\n",
    "    \n",
    "    tokens = line.split()    \n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in en_stop]\n",
    "    #tokens = [word for word in tokens if len(word) > 3]\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "def preprocess_all(lyrics):\n",
    "    ret = []\n",
    "    for line in lyrics:\n",
    "        tokens = preprocess(line)\n",
    "        if not tokens == []:\n",
    "            ret.append(tokens)\n",
    "            \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then apply the `preprocess_all` function on the `'lyrics'` column of the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_lyrics'] = df['lyrics'].apply(preprocess_all)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have preprocessed all the lyrics for all available songs, we move on to representation. For this project, we decided to go with word embeddings, specifically Google's pre-trained FastText model, which can be downloaded at the link below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Fastext Model](https://code.google.com/archive/p/word2vec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model, which is about three gigabytes large, was trained on Google News's corpus text, and is a basic pre-trained word embedding models to work with and is widely used throughout all sorts of text preprocessing projects. We have the code below to load the model from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some time, the model will have been loaded. We can then move on to the word embeddings for each lyric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, regarding preprocessing, we have a few choices. We have that the basic structure of each preprocessed lyric is in the form of a list of lines of the lyrics, with each line being a list of words\n",
    "\n",
    "By default, FastText produces a vector of length 300 for each input word. If we choose to take the route of no pooling whatsoever, our embedding would be a 3-dimensional vector for each lyric, in which each line in the lyric would transformed into a variable length 2-dimensional vector with width 300. Not to mention the padding to account for differences in lengths of lines, and then diferences in number of lines between lyrics, we end up with an inordinate number of features. For regression, we would have to flatten this 3-dimensional vector into a 1-dimensional one, which could have length over 50,000. For linear and logistic regression, this would not only be an inordinate number of features, but could also lead to a long runtime for producing weights.\n",
    "\n",
    "What we can do instead is take the heuristic of keeping everything simple. Instead of a feature size based on the maximum lengths as would be implemented in the above design, to avoid complication and padding, we can keep the final vector length at 300. We can achieve this in a number of techniques, including a max pool of sorts, or some type of mask applied to reduce dimensionality. What we decided to do was to take averages of all words in a line, and take averages of all lines. To be sure, there is likely considerable information loss in using such a technique, but being able to, in some small way, include every single word in the final embedding without compromising simplicity was the motivation for choosing this approach.\n",
    "\n",
    "We have, in the fashion described above, the code below which creates the word embeddings for each line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line2vec(line):\n",
    "    line_embedding = np.zeros(300)\n",
    "    exceptions = 0\n",
    "    for word in line:\n",
    "        #print(model[word].shape)\n",
    "        try:\n",
    "            line_embedding += model[word]\n",
    "        except KeyError:\n",
    "            #print(word)\n",
    "            exceptions += 1\n",
    "    \n",
    "    return line_embedding / (len(line) - exceptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply the function to transform each lyric into an embedding, and then stack all available embeddings into a 2-dimensional vector, to be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_embeddings = np.zeros((1, 300))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    lyrics = row['preprocessed_lyrics']\n",
    "    \n",
    "    if index % 500 == 0:\n",
    "        print(index)\n",
    "    \n",
    "    lyric_embedding = np.zeros(300)\n",
    "    for line in lyrics:\n",
    "        lyric_embedding += line2vec(line)\n",
    "        \n",
    "    lyric_embedding /= len(lyrics)\n",
    "    \n",
    "    lyric_embedding = np.reshape(lyric_embedding, (1, -1))\n",
    "    \n",
    "    lyrics_embeddings = np.concatenate((lyrics_embeddings, lyric_embedding), axis=0)\n",
    "    \n",
    "lyrics_embeddings = np.delete(lyrics_embeddings, [0], axis=0)\n",
    "\n",
    "np.save(\"updated_embeddings.npy\",lyrics_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing both the lyrics and the genres, we can move onto our analysis techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre Classification\n",
    "\n",
    "We use the genre information from the song to map created earlier as labels in the dataset.\n",
    "We first look at properties for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(\"updated_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5250, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "label_to_names = {}\n",
    "with open(\"updated_song_map.csv\",\"r\") as sm:\n",
    "    reader = csv.DictReader(sm,delimiter=\",\")\n",
    "    for row in reader:\n",
    "        genre = row[\"genre_temp\"]\n",
    "        val = row[\"genre_map\"]\n",
    "        labels.append(val)\n",
    "        label_to_names[val] = genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'Pop',\n",
       " '5': 'Rock',\n",
       " '2': 'Folk, World, & Country',\n",
       " '3': 'Electronic',\n",
       " '1': 'Hip Hop',\n",
       " '4': 'Funk / Soul'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0': 1133, '5': 924, '2': 963, '3': 742, '1': 943, '4': 545})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5250"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data and train/test split\n",
    "\n",
    "Some of the values in the embedding is nan or missing. These are replaced by mean values of that particular column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer()\n",
    "imp_mean.fit(embeddings)\n",
    "embeddings = imp_mean.transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split(\n",
    "    embeddings, labels, test_size=0.1, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4725, 300), (525, 300))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape,test_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also normalize all values.\n",
    "\n",
    "Subtract mean and scale to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(train_img)\n",
    "# Apply transform to both the training set and the test set.\n",
    "train_img = scaler.transform(train_img)\n",
    "test_img = scaler.transform(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ 'Logistic Regression', 'Linear SVM', 'Random']\n",
    "accuracies = []\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of classification techniques\n",
    "\n",
    "We will compare three models for classification.\n",
    "A random classifier for a baseline.\n",
    "\n",
    "Logistic regression in a multinomial setting\n",
    "\n",
    "Linear SVM with a multinomial setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=10000, multi_class='multinomial', n_jobs=None,\n",
       "                   penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "                   verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e5,solver='lbfgs',multi_class=\"multinomial\",max_iter=10000)\n",
    "logreg.fit(train_img, train_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 336\n",
      "Accuracy: 0.36\n",
      "F1_score: 0.32\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# use the model to make predictions with the test data\n",
    "y_pred = logreg.predict(test_img)\n",
    "# how did our model perform?\n",
    "count_misclassified = (test_lbl != y_pred).sum()\n",
    "print('Misclassified samples: {}'.format(count_misclassified))\n",
    "accuracy = metrics.accuracy_score(test_lbl, y_pred)\n",
    "print('Accuracy: {:.2f}'.format(accuracy))\n",
    "F1_score = metrics.f1_score(test_lbl, y_pred,average=\"macro\")\n",
    "print('F1_score: {:.2f}'.format(F1_score))\n",
    "\n",
    "accuracies.append(accuracy)\n",
    "f1_scores.append(F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=10000,\n",
       "          multi_class='ovr', penalty='l2', random_state=0, tol=1e-05,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=0, tol=1e-5,multi_class=\"ovr\",max_iter=10000)\n",
    "clf.fit(train_img,train_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 327\n",
      "Accuracy: 0.38\n",
      "F1_score: 0.33\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(test_img)\n",
    "# how did our model perform?\n",
    "count_misclassified = (test_lbl != y_pred).sum()\n",
    "print('Misclassified samples: {}'.format(count_misclassified))\n",
    "accuracy = metrics.accuracy_score(test_lbl, y_pred)\n",
    "print('Accuracy: {:.2f}'.format(accuracy))\n",
    "F1_score = metrics.f1_score(test_lbl, y_pred,average=\"macro\")\n",
    "print('F1_score: {:.2f}'.format(F1_score))\n",
    "accuracies.append(accuracy)\n",
    "f1_scores.append(F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_clf = DummyClassifier(strategy=\"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 444\n",
      "Accuracy: 0.15\n",
      "F1_score: 0.15\n"
     ]
    }
   ],
   "source": [
    "dummy_clf.fit(train_img,train_lbl)\n",
    "y_pred = dummy_clf.predict(test_img)\n",
    "# how did our model perform?\n",
    "count_misclassified = (test_lbl != y_pred).sum()\n",
    "print('Misclassified samples: {}'.format(count_misclassified))\n",
    "accuracy = metrics.accuracy_score(test_lbl, y_pred)\n",
    "print('Accuracy: {:.2f}'.format(accuracy))\n",
    "F1_score = metrics.f1_score(test_lbl, y_pred,average=\"macro\")\n",
    "print('F1_score: {:.2f}'.format(F1_score))\n",
    "accuracies.append(accuracy)\n",
    "f1_scores.append(F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, accuracies, width, label='Accuracy')\n",
    "rects2 = ax.bar(x + width/2, f1_scores, width, label='F1_Score')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Accuracy and F1 Scores')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

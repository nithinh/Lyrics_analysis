{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a86433331f63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasttext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gensim\n",
    "from gensim.models.fasttext import FastText\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"Lyrics_analysis-master\\Lyrics_analysis-master\\songs.json\", 'r') as f:\n",
    "    songs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def max_len(lyrics):\n",
    "    max_len = 0\n",
    "    for i in range(len(lyrics)):\n",
    "        print(lyrics)\n",
    "        if len(lyrics[i]['lyrics']) > max_len:\n",
    "            max_len = len(lyrics[i]['lyrics'])\n",
    "            \n",
    "    return max_len'''\n",
    "\n",
    "print(len(songs))\n",
    "print(songs[0])\n",
    "limit = len(songs)\n",
    "\n",
    "bad = []\n",
    "\n",
    "max_len = 0\n",
    "song = \"\"\n",
    "\n",
    "for i in range(limit):\n",
    "    try:\n",
    "        if len(songs[i]['lyrics']) > max_len:\n",
    "            max_len = len(songs[i]['lyrics'])\n",
    "            song = songs[i]\n",
    "    except TypeError:\n",
    "        bad.append(i)\n",
    "        max_len = max_len\n",
    "    except KeyError:\n",
    "        bad.append(i)\n",
    "        max_len = max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "lyrics = []\n",
    "bad2 = []\n",
    "\n",
    "for i in range(limit):\n",
    "    try:\n",
    "        titles.append(songs[i]['title'])\n",
    "        lyrics.append(songs[i]['lyrics'])\n",
    "    except TypeError:\n",
    "        bad2.append(i)\n",
    "    except KeyError:\n",
    "        bad2.append(i)\n",
    "        max_len = max_len\n",
    "        \n",
    "assert(bad2 == bad)\n",
    "\n",
    "df = pd.DataFrame({'songs' : titles, 'lyrics' : lyrics})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def preprocess(line):  \n",
    "    \n",
    "    line = re.sub(r'\\W', ' ', str(line))\n",
    "    line = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', line)\n",
    "    line = re.sub(r'\\^[a-zA-Z]\\s+', ' ', line)\n",
    "    line = re.sub(r'\\s+', ' ', line, flags=re.I)\n",
    "    \n",
    "    line = line.lower()\n",
    "    \n",
    "    tokens = line.split()    \n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in en_stop]\n",
    "    #tokens = [word for word in tokens if len(word) > 3]\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "def preprocess_all(lyrics):\n",
    "    ret = []\n",
    "    for line in lyrics:\n",
    "        tokens = preprocess(line)\n",
    "        if not tokens == []:\n",
    "            ret.append(tokens)\n",
    "            \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_lyrics'] = df['lyrics'].apply(preprocess_all)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for index, row in df.iterrows():\n",
    "    lyrics = row['preprocessed_lyrics']\n",
    "    if len(lyrics) > max_len:\n",
    "        max_len = len(lyrics)\n",
    "        \n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "window_size = 40\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n",
    "\n",
    "'''ft_model = FastText(size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      sg=0,\n",
    "                      iter=100)'''\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_embeddings = np.zeros((1, 300))\n",
    "\n",
    "def line2vec(line):\n",
    "    line_embedding = np.zeros(300)\n",
    "    exceptions = 0\n",
    "    for word in line:\n",
    "        #print(model[word].shape)\n",
    "        try:\n",
    "            line_embedding += model[word]\n",
    "        except KeyError:\n",
    "            #print(word)\n",
    "            exceptions += 1\n",
    "    \n",
    "    return line_embedding / (len(line) - exceptions)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    lyrics = row['preprocessed_lyrics']\n",
    "    \n",
    "    if index % 500 == 0:\n",
    "        print(index)\n",
    "    \n",
    "    lyric_embedding = np.zeros(300)\n",
    "    for line in lyrics:\n",
    "        lyric_embedding += line2vec(line)\n",
    "        \n",
    "    lyric_embedding /= len(lyrics)\n",
    "    \n",
    "    lyric_embedding = np.reshape(lyric_embedding, (1, -1))\n",
    "    #print(lyrics_embeddings.shape)\n",
    "    #print(lyric_embedding.shape)\n",
    "    lyrics_embeddings = np.concatenate((lyrics_embeddings, lyric_embedding), axis=0)\n",
    "    \n",
    "print(len(lyrics))\n",
    "lyrics_embeddings = np.delete(lyrics_embeddings, [0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embeddings.npy', lyrics_embeddings)\n",
    "lyrics_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load('embeddings.npy')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
